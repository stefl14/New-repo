Project Description

The aims of this project is to classify elements of a list of named entities males, females or businesss of specific types. For example, "John Smith" would be classified as a male and "Johnson and Fowler Car Washing" would be classified as a partnership in an ideal world. While this may seem like an arbitrary classification task, its goal is to engineer features for another project. The project in question attempts to predict the probability of entities (e.g. homeowners, landlords or businesses) complying with fines for failing to maintain the appearance of their properties in Detroit. This is useful because it gives the city better information about which violators it should focus its resources on when chasing fines. One of the features in the dataset for this project is the violator's name. Unfortunately, there is no type associated with these names. That is, there is no indication as to whether "Jane Doe" is a female or a multinational corporation, even though an entity's type is likely to be highly informative about whether or not it is likely to comply with a fine. As a result, a highly accurate model for classifying entities based solely on short strings of text (the names) is likely to be of use for the predictive success of the project. Manually labelling the features is not feasible because there are circa 200,000 rows.
A carefully designed neural network might achieve superhuman success in this task. Without domain specific knowledge, it is unlikely that a person will be able to separate businesses into highly specific categories as well as a high performance network can. However, in the case of determining gender from names even a relatively sophisticated model might not outperform people. Virtually everyone has domain specific knowledge in determining gender from names. We do it all the time in our day to day life. It is also extremely difficult for a model to generalise to unseen data to determine gender because, in many cases, the "rules" that determine whether an individual is male or female are violated. In addition, in some cases it is impossible to predict name from gender with high probability because of unisex names. Given relatively limited training data (explained below), this means our expectations should be somewhat tempered when it comes to categorising people as male or female. Humans can rely on their vast experience to identify exceptions to rules, something a neural network may not be able to do given the limited size of our training data (explained below).

Datasets

The ambitious nature of the task at hand is further amplified by the fact that the training data is unlikely to be representative of the actual data we want to predict on. The data we want to predict on contains (for the most part) both first names and last names for cases in which the violating entities are people. Since the data to predict on has a mixture of entity types with no labels, it is not possible to eliminate last names from the dataset. Because of the lack of labels, doing so would also delete important tokens from non name entities. It was therefore necessary to source data containing both first and last names along with their gender labels. This naturally limited the size and representativeness of the training dataset. In the end, data from US prison inmates was used for names. This data is limited in size and is unlikely to be representative of the Detroit blight violator population as a whole for many reasons, including the fact that the inmate population is not representative of society at large.
The training data for business names comes from Iowa, where there may be subtle naming convention differences compared to Detroit. The data contains the names of registered businesses and their types. All of the data issues mentioned thus far are likely to reduce the ability of the model to generalise to the data to predict on. Performance on the unseen test set will therefore be an overestimate of performance in the application that matters for our problem.
Generalisation performance will be made more difficult still by formatting issues. For example, some of the name strings in the data we want to predict on contains middle names and some don't. It is not possible to remove middle names because we don't have labels for which entities are people and which are businesses. It is therefore difficult to train on data of the same format of the names we actually see in practice. This is a big issue, since if we remove middle names from the training data, the model may simply predict that entities with more than two words are businesses. On the contrary, if we don't remove middle names, there may be insufficient training examples of names with no middle name, in which case any name with only two words in the dataset may be predicted to be a business. The solution used here involved inserting a random letter to indicate the presence of a middle name into the training data. This is an imperfect solution for a number of reasons, but should work well to combat the issues outlined above.

Model Architecture

This architecture of this model was inspired by a 2017 paper used to determine nationalities from names. The model creates character level unigram, bigrams and trigram sequences from names, learns word embeddings from the tokens in these sequences and passes the embedded sequences into three separate LSTM networks, the results of which are concatenated and passed through a dense layer to determine the class of each word. More details of the approach used can be found in the paper or by following along with the project here. The paper used for inspiration and a source for adaptation can be found at the following link:
https://www.ijcai.org/proceedings/2017/0289.pdf
Note that there are multiple differences from the paper in the Keras adapted version used here, but these are not overly important.

Model Performance

Although we don't have labels for the column to predict on, simple observation of a medium size sample shows that the model performs relatively poorly. A very high proportion of data points can easily be identified by a human as misclassified, despite the very good performance on an unseen test set. Ultimately, converting the column using this classifier was not able to improve performance on the blight prediction task and so was not used. This is likely due to all of the issues outlined above, which made the project extremely ambitious. Hyperparameter tuning was not carried out due to limitations in both time and computing power but also because it is unlikely that it would have improved matters: the base level performance was fairly good but the performance on the application data was far worse, thus indicating that the problems above were somewhat insurmountable.
Despite difficulties engineering useful features in practice with this model, the project proved useful in demonstrating how to adapt concepts from an academic paper into a Keras multi input neural network. Though the network in the 2017 paper aimed to identify nationality from names, the strong performance of a similar model on the test set here in a different problem domain demonstrates the usefulness of the methods and shows that the concepts used are likely applicable to many other text classification problems. The lack of success in generating useful features likely stems from the very specific nature of the underlying data to predict on and an inability to find similar training data. The model itself, however, is very successful. To demonstrate the usefulness of the model, its performance is compared with a simplistic out of the box gradient boosting model. Though no complex feature engineering or tuning was carried out on the gradient boosting model, the difference in performance is a strong indicator that the complex model is a good idea, especially given that tuning often does little for performance in a gradient boosting model.

